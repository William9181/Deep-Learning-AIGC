import streamlit as st
import pandas as pd
import plotly.express as px
import torch
import torch.nn.functional as F
# âœ… ä¿®æ”¹å¾Œçš„å¯«æ³• (æ‹†é–‹ä¾†åŒ¯å…¥)
from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding
# é—œéµï¼šç›´æ¥å¾å­æ¨¡çµ„åŒ¯å…¥ Trainerï¼Œé¿é–‹ transformers ä¸»å…¥å£çš„æª¢æŸ¥
from transformers.trainer import Trainer
from transformers.training_args import TrainingArguments
from transformers.trainer_callback import TrainerCallback
from datasets import load_dataset
from peft import PeftModel, LoraConfig, IA3Config, TaskType, get_peft_model
import os
import shutil
import time

# è¨­å®šé é¢é…ç½®
st.set_page_config(page_title="IMDb PEFT Project", page_icon="ğŸ¬", layout="wide")

# åˆå§‹åŒ– Session State
if "custom_model_path" not in st.session_state:
    st.session_state["custom_model_path"] = None
if "custom_model_name" not in st.session_state:
    st.session_state["custom_model_name"] = ""

# --- å´é‚Šæ¬„å°èˆª ---
st.sidebar.title("åŠŸèƒ½å°èˆª")
app_mode = st.sidebar.radio(
    "é¸æ“‡åŠŸèƒ½é é¢", 
    ["ğŸ“ å°ˆæ¡ˆæ‘˜è¦", "ğŸ› ï¸ ç·šä¸Šè¨“ç·´å¯¦é©—å®¤", "ğŸ¬ æ¨¡å‹æ¨è«–æ¼”ç¤º", "ğŸ“Š åƒæ•¸é‡è¦–è¦ºåŒ–åˆ†æ"]
)

# ===========================
# å·¥å…·é¡åˆ¥: Streamlit è¨“ç·´å›èª¿
# ===========================
class StreamlitLogCallback(TrainerCallback):
    def __init__(self, progress_bar, status_text, total_steps):
        self.progress_bar = progress_bar
        self.status_text = status_text
        self.total_steps = total_steps

    def on_step_end(self, args, state, control, **kwargs):
        current_step = state.global_step
        if self.total_steps > 0:
            progress = min(current_step / self.total_steps, 1.0)
            self.progress_bar.progress(progress)
            loss_log = "..."
            if state.log_history:
                for log in reversed(state.log_history):
                    if "loss" in log:
                        loss_log = f"{log['loss']:.4f}"
                        break
            self.status_text.text(f"Training... Step {current_step}/{self.total_steps} (Loss: {loss_log})")

# ===========================
# å…±ç”¨å‡½å¼: æ¨¡å‹è¼‰å…¥
# ===========================
@st.cache_resource
def load_model_pipeline(method_name, custom_path=None):
    base_model_name = "bert-base-uncased"
    if method_name == "Custom Trained Model":
        peft_model_id = custom_path
    else:
        model_paths = {
            "LoRA": "bert-lora-imdb-final",
            "IA3": "bert-ia3-imdb-final"
        }
        peft_model_id = model_paths.get(method_name)
    
    if not peft_model_id or not os.path.exists(peft_model_id):
        if method_name in ["LoRA", "IA3"]:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹è³‡æ–™å¤¾ï¼š{peft_model_id}ã€‚è«‹å…ˆåŸ·è¡Œ compare_peft.pyã€‚")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è‡ªå®šç¾©æ¨¡å‹è·¯å¾‘ï¼š{peft_model_id}")

    with st.spinner(f"æ­£åœ¨è¼‰å…¥ {method_name}..."):
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name, num_labels=2,
            id2label={0: "NEGATIVE", 1: "POSITIVE"}, label2id={"NEGATIVE": 0, "POSITIVE": 1}
        )
        model = PeftModel.from_pretrained(base_model, peft_model_id)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
    return tokenizer, model, device

# ===========================
# é é¢ 1: å°ˆæ¡ˆæ‘˜è¦ (æ–°å¢é é¢)
# ===========================
if app_mode == "ğŸ“ å°ˆæ¡ˆæ‘˜è¦":
    st.title("ğŸ“ å°ˆæ¡ˆæˆæœæ‘˜è¦")
    st.markdown("### é«˜æ•ˆåƒæ•¸å¾®èª¿ (PEFT) æ–¼ IMDb æƒ…ç·’åˆ†æä¹‹æ‡‰ç”¨")
    st.markdown("---")
    
    # ä½¿ç”¨ Container è®“æ’ç‰ˆæ›´æ¼‚äº®
    with st.container():
        st.markdown("""
        #### ğŸ“Œ å°ˆæ¡ˆæ¦‚è¿°
        æœ¬å°ˆæ¡ˆæ—¨åœ¨æ¢è¨ **é«˜æ•ˆåƒæ•¸å¾®èª¿æŠ€è¡“ (Parameter-Efficient Fine-Tuning, PEFT)** åœ¨è‡ªç„¶èªè¨€è™•ç†ä»»å‹™ä¸­çš„æ‡‰ç”¨æ•ˆç›Šã€‚æˆ‘å€‘é¸ç”¨ **BERT-base** é è¨“ç·´æ¨¡å‹ï¼Œé‡å° **IMDb é›»å½±è©•è«–è³‡æ–™é›†** é€²è¡ŒäºŒå…ƒæƒ…ç·’åˆ†é¡ï¼ˆæ­£é¢/è² é¢ï¼‰ä»»å‹™ã€‚
        
        #### ğŸ§ª æŠ€è¡“èˆ‡æ–¹æ³•
        å¯¦é©—æ ¸å¿ƒåœ¨æ–¼å°æ¯”å‚³çµ±çš„ã€Œå…¨é‡å¾®èª¿ (Full Fine-tuning)ã€èˆ‡å…©ç¨®ä¸»æµ PEFT æŠ€è¡“ï¼š
        * **LoRA (Low-Rank Adaptation)**ï¼šé€éä½ç§©çŸ©é™£åˆ†è§£ï¼Œåƒ…è¨“ç·´æ—è·¯åƒæ•¸ã€‚
        * **IA3**ï¼šé€éæŠ‘åˆ¶èˆ‡æ”¾å¤§å…§éƒ¨æ¿€æ´»å€¼å‘é‡é€²è¡Œå¾®èª¿ã€‚
        
        #### ğŸ“Š å¯¦é©—çµè«–
        å¯¦é©—çµæœè­‰å¯¦ï¼Œåœ¨è³‡æºå—é™çš„ç’°å¢ƒä¸‹ï¼ŒPEFT æŠ€è¡“å±•ç¾äº†æ¥µå¤§çš„å„ªå‹¢ï¼š
        1.  **æ¥µè‡´è¼•é‡åŒ–**ï¼šç›¸è¼ƒæ–¼å…¨é‡å¾®èª¿ï¼ˆç´„ 1.1 å„„åƒæ•¸ï¼‰ï¼Œ**LoRA åƒ…éœ€è¨“ç·´ç´„ 0.27%** çš„åƒæ•¸ï¼Œè€Œ **IA3 æ›´é€²ä¸€æ­¥é™è‡³ 0.05%**ï¼Œå¤§å¹…é™ä½äº†å„²å­˜èˆ‡è¨˜æ†¶é«”éœ€æ±‚ã€‚
        2.  **æ•ˆèƒ½å„ªç•°**ï¼šåœ¨å¤§å¹…æ¸›å°‘åƒæ•¸é‡çš„æƒ…æ³ä¸‹ï¼ŒPEFT æ¨¡å‹ä»èƒ½é”åˆ°èˆ‡å…¨é‡å¾®èª¿ç›¸ç•¶çš„é æ¸¬æº–ç¢ºç‡ã€‚
        
        #### ğŸ’» å¹³å°åŠŸèƒ½
        æœ¬ç³»çµ±æ•´åˆ Streamlit æ§‹å»ºäº†å®Œæ•´çš„äº’å‹•å¼å¯¦é©—å¹³å°ï¼ŒåŒ…å«ï¼š
        * **ç·šä¸Šè¨“ç·´å¯¦é©—å®¤**ï¼šå…è¨±ä½¿ç”¨è€…å³æ™‚èª¿æ•´åƒæ•¸ï¼ˆå¦‚ Rank, Learning Rateï¼‰ä¸¦è¨“ç·´å®¢è£½åŒ–æ¨¡å‹ã€‚
        * **æ¨¡å‹æ¨è«–æ¼”ç¤º**ï¼šæä¾›å³æ™‚æ–‡æœ¬è¼¸å…¥èˆ‡æƒ…ç·’ä¿¡å¿ƒæŒ‡æ•¸åˆ†æã€‚
        * **è¦–è¦ºåŒ–åˆ†æ**ï¼šåˆ©ç”¨å°æ•¸å°ºåº¦åœ–è¡¨ï¼Œç›´è§€å‘ˆç¾åƒæ•¸é‡æ•¸å€‹æ•¸é‡ç´šçš„ç¸®æ¸›å·®ç•°ã€‚
        """)
        
    st.info("ğŸ‘ˆ è«‹é»æ“Šå·¦å´å°èˆªæ¬„ä½ï¼Œé–‹å§‹é«”é©—å„é …åŠŸèƒ½ã€‚")

# ===========================
# é é¢ 2: ç·šä¸Šè¨“ç·´å¯¦é©—å®¤
# ===========================
elif app_mode == "ğŸ› ï¸ ç·šä¸Šè¨“ç·´å¯¦é©—å®¤":
    st.title("ğŸ› ï¸ ç·šä¸Šè¨“ç·´å¯¦é©—å®¤")
    st.markdown("åœ¨æ­¤é é¢ï¼Œæ‚¨å¯ä»¥èª¿æ•´åƒæ•¸ä¸¦**å³æ™‚è¨“ç·´**ä¸€å€‹è¼•é‡ç´šæ¨¡å‹ã€‚")
    st.info("ğŸ’¡ æç¤ºï¼šæ­¤å¯¦é©—å®¤ä½¿ç”¨æ¥µå°‘é‡æ•¸æ“š (Sample) é€²è¡Œå¿«é€Ÿæ¼”ç¤ºã€‚")
    
    col_conf, col_param = st.columns(2)
    with col_conf:
        st.subheader("1. æ¨¡å‹æ¶æ§‹è¨­å®š")
        peft_type = st.selectbox("é¸æ“‡ PEFT æ–¹æ³•", ["LoRA", "IA3"])
        if peft_type == "LoRA":
            r_rank = st.slider("LoRA Rank (r)", 4, 32, 8, 4)
            lora_alpha = st.slider("LoRA Alpha", 8, 64, 16, 8)
            dropout = st.slider("Dropout", 0.0, 0.5, 0.1)
        else:
            st.info("IA3 ä¸éœ€è¦è¨­å®š Rankã€‚")
            r_rank = 0
            
    with col_param:
        st.subheader("2. è¨“ç·´åƒæ•¸è¨­å®š")
        lr_default = 2e-4 if peft_type=="LoRA" else 5e-3
        learning_rate = st.number_input("Learning Rate", value=lr_default, format="%.5f")
        epochs = st.slider("Epochs", 1, 5, 1)
        batch_size = st.selectbox("Batch Size", [8, 16, 32], index=0)
        sample_size = st.slider("è¨“ç·´æ¨£æœ¬æ•¸", 20, 200, 50)

    st.markdown("---")
    if st.button("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹", type="primary"):
        status_area = st.empty()
        progress_bar = st.progress(0)
        try:
            with st.spinner("æ­£åœ¨åˆå§‹åŒ–..."):
                model_checkpoint = "bert-base-uncased"
                tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
                dataset = load_dataset("imdb")
                small_train = dataset["train"].shuffle(seed=int(time.time())).select(range(sample_size))
                
                def preprocess_function(examples):
                    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)
                
                tokenized_train = small_train.map(preprocess_function, batched=True)
                data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
                base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)
                
                if peft_type == "LoRA":
                    peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=r_rank, lora_alpha=lora_alpha, lora_dropout=dropout, target_modules=["query", "value"])
                else:
                    peft_config = IA3Config(task_type=TaskType.SEQ_CLS, target_modules=["key", "value", "output.dense"], feedforward_modules=["output.dense"])
                
                model = get_peft_model(base_model, peft_config)
                output_dir = f"./custom_trained_{peft_type}"
                total_steps = (len(small_train) // batch_size) * epochs
                if total_steps == 0: total_steps = epochs
                
                training_args = TrainingArguments(
                    output_dir=output_dir, learning_rate=learning_rate, per_device_train_batch_size=batch_size,
                    num_train_epochs=epochs, weight_decay=0.01, logging_steps=1, save_strategy="no", report_to="none",
                    use_cpu=not torch.cuda.is_available()
                )
                
                trainer = Trainer(
                    model=model, args=training_args, train_dataset=tokenized_train, tokenizer=tokenizer,
                    data_collator=data_collator, callbacks=[StreamlitLogCallback(progress_bar, status_area, total_steps)]
                )

            status_area.text("Training started...")
            trainer.train()
            save_path = "./custom_user_model"
            if os.path.exists(save_path): shutil.rmtree(save_path)
            model.save_pretrained(save_path)
            st.session_state["custom_model_path"] = save_path
            st.session_state["custom_model_name"] = f"Custom {peft_type} (Sample={sample_size}, Epochs={epochs})"
            progress_bar.progress(100)
            status_area.success(f"è¨“ç·´å®Œæˆï¼è«‹å‰å¾€ã€ŒğŸ¬ æ¨¡å‹æ¨è«–æ¼”ç¤ºã€é é¢æ¸¬è©¦ã€‚")
            st.balloons()
        except Exception as e:
            st.error(f"éŒ¯èª¤: {e}")

# ===========================
# é é¢ 3: æ¨¡å‹æ¨è«–æ¼”ç¤º
# ===========================
elif app_mode == "ğŸ¬ æ¨¡å‹æ¨è«–æ¼”ç¤º":
    st.title("ğŸ¬ IMDb é›»å½±è©•è«–æƒ…ç·’åˆ†æ")
    st.markdown("---")
    st.sidebar.header("æ¨è«–æ¨¡å‹é¸æ“‡")
    available_methods = []
    if os.path.exists("bert-lora-imdb-final"): available_methods.append("LoRA")
    if os.path.exists("bert-ia3-imdb-final"): available_methods.append("IA3")
    if st.session_state["custom_model_path"] and os.path.exists(st.session_state["custom_model_path"]):
        available_methods.append("Custom Trained Model")
        st.sidebar.success("âœ… åµæ¸¬åˆ°æ‚¨å‰›è¨“ç·´çš„æ¨¡å‹ï¼")

    if not available_methods:
        st.error("æ‰¾ä¸åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ã€‚è«‹å…ˆåŸ·è¡Œ compare_peft.py æˆ–åœ¨ç·šä¸Šå¯¦é©—å®¤è¨“ç·´ã€‚")
        st.stop()

    selected_method = st.sidebar.selectbox("é¸æ“‡å¾®èª¿æ¨¡å‹", available_methods)
    if selected_method == "Custom Trained Model":
        st.sidebar.caption(f"åƒæ•¸: {st.session_state['custom_model_name']}")

    try:
        custom_path = st.session_state["custom_model_path"] if selected_method == "Custom Trained Model" else None
        tokenizer, model, device = load_model_pipeline(selected_method, custom_path)
        st.sidebar.info(f"è£ç½®: {device.upper()}")
    except Exception as e:
        st.error(f"è¼‰å…¥å¤±æ•—: {e}")
        st.stop()

    if "review_input" not in st.session_state: st.session_state["review_input"] = ""
    col1, col2 = st.columns(2)
    with col1:
        if st.button("å¸¶å…¥æ­£é¢ç¯„ä¾‹"): st.session_state["review_input"] = "The cinematography was breathtaking and the story was deeply moving. I loved every minute of it!"
    with col2:
        if st.button("å¸¶å…¥è² é¢ç¯„ä¾‹"): st.session_state["review_input"] = "The plot made no sense and the acting was wooden. Total waste of time."

    user_input = st.text_area("è«‹è¼¸å…¥å½±è©•ï¼š", height=150, key="review_input")
    if st.button("é–‹å§‹åˆ†æ", type="primary") and user_input.strip():
        with st.spinner("åˆ†æä¸­..."):
            inputs = tokenizer(user_input, return_tensors="pt", truncation=True, max_length=512).to(device)
            with torch.no_grad(): outputs = model(**inputs)
            probs = F.softmax(outputs.logits, dim=-1)
            pred_id = outputs.logits.argmax().item()
            label = model.config.id2label[pred_id]
            confidence = probs[0][pred_id].item()
            prob_neg, prob_pos = probs[0][0].item(), probs[0][1].item()

        if label == "POSITIVE": st.success(f"ğŸ‰ æ­£é¢ (ä¿¡å¿ƒæŒ‡æ•¸: {confidence:.2%})")
        else: st.error(f"ğŸ˜ è² é¢ (ä¿¡å¿ƒæŒ‡æ•¸: {confidence:.2%})")
        
        c1, c2 = st.columns(2)
        with c1: st.write("**Negative**"); st.progress(prob_neg); st.caption(f"{prob_neg:.1%}")
        with c2: st.write("**Positive**"); st.progress(prob_pos); st.caption(f"{prob_pos:.1%}")

# ===========================
# é é¢ 4: åƒæ•¸é‡è¦–è¦ºåŒ–åˆ†æ
# ===========================
elif app_mode == "ğŸ“Š åƒæ•¸é‡è¦–è¦ºåŒ–åˆ†æ":
    st.title("ğŸ“Š PEFT åƒæ•¸é‡ç˜¦èº«æˆæœå±•ç¤º")
    st.markdown("---")
    csv_file = "peft_comparison_results.csv"
    if not os.path.exists(csv_file):
        st.warning(f"æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶ {csv_file}ã€‚è«‹ç¢ºä¿æ‚¨å·²æˆåŠŸåŸ·è¡Œ compare_peft.pyã€‚")
    else:
        df = pd.read_csv(csv_file)
        df["Formatted Params"] = df["Trainable Params"].apply(lambda x: f"{x:,}")
        
        st.subheader("1. å¯è¨“ç·´åƒæ•¸é‡å°æ¯” (å°æ•¸å°ºåº¦)")
        fig_log = px.bar(df, x="Method", y="Trainable Params", color="Method", text="Formatted Params",
            title="Trainable Parameters (Log Scale)", log_y=True,
            color_discrete_map={"Full Fine-tuning (Baseline)": "lightgrey", "LoRA": "#636EFA", "IA3": "#EF553B"}, height=500)
        fig_log.update_traces(textposition='outside')
        st.plotly_chart(fig_log, use_container_width=True)

        st.subheader("2. PEFT æ–¹æ³•å…§éƒ¨å°æ±º")
        df_peft = df[df["Method"] != "Full Fine-tuning (Baseline)"]
        if not df_peft.empty:
            fig_linear = px.bar(df_peft, x="Trainable Params", y="Method", color="Method", text="Formatted Params",
                title="LoRA vs. IA3 (Linear Scale)", orientation='h',
                color_discrete_map={"LoRA": "#636EFA", "IA3": "#EF553B"}, height=400)
            fig_linear.update_traces(textposition='outside')
            st.plotly_chart(fig_linear, use_container_width=True)
        

        st.markdown("### è©³ç´°æ•¸æ“š"); st.dataframe(df)
